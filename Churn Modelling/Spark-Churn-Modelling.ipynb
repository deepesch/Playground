{"cells":[{"cell_type":"markdown","source":["# Load the data\n\nWe need to load data from a file in to a Spark DataFrame.\nEach row is an observed customer, and each column contains\nattributes of that customer.\n\n    Fields:\n    state: discrete.\n    account length: numeric.\n    area code: numeric.\n    phone number: discrete.\n    international plan: discrete.\n    voice mail plan: discrete.\n    number vmail messages: numeric.\n    total day minutes: numeric.\n    total day calls: numeric.\n    total day charge: numeric.\n    total eve minutes: numeric.\n    total eve calls: numeric.\n    total eve charge: numeric.\n    total night minutes: numeric.\n    total night calls: numeric.\n    total night charge: numeric.\n    total intl minutes: numeric.\n    total intl calls: numeric.\n    total intl charge: numeric.\n    number customer service calls: numeric.\n    churned: discrete.\n\n'Numeric' and 'discrete' do not adequately describe the fundamental differencecs in the attributes."],"metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n\nsqlContext = SQLContext(sc)\nschema = StructType([ \\\n    StructField(\"state\", StringType(), True), \\\n    StructField(\"account_length\", DoubleType(), True), \\\n    StructField(\"area_code\", StringType(), True), \\\n    StructField(\"phone_number\", StringType(), True), \\\n    StructField(\"intl_plan\", StringType(), True), \\\n    StructField(\"voice_mail_plan\", StringType(), True), \\\n    StructField(\"number_vmail_messages\", DoubleType(), True), \\\n    StructField(\"total_day_minutes\", DoubleType(), True), \\\n    StructField(\"total_day_calls\", DoubleType(), True), \\\n    StructField(\"total_day_charge\", DoubleType(), True), \\\n    StructField(\"total_eve_minutes\", DoubleType(), True), \\\n    StructField(\"total_eve_calls\", DoubleType(), True), \\\n    StructField(\"total_eve_charge\", DoubleType(), True), \\\n    StructField(\"total_night_minutes\", DoubleType(), True), \\\n    StructField(\"total_night_calls\", DoubleType(), True), \\\n    StructField(\"total_night_charge\", DoubleType(), True), \\\n    StructField(\"total_intl_minutes\", DoubleType(), True), \\\n    StructField(\"total_intl_calls\", DoubleType(), True), \\\n    StructField(\"total_intl_charge\", DoubleType(), True), \\\n    StructField(\"number_customer_service_calls\", DoubleType(), True), \\\n    StructField(\"churned\", StringType(), True)])\n\nchurn_df = sqlContext.read \\\n    .format('com.databricks.spark.csv') \\\n    .load(\"/FileStore/tables/fyarcesz1457233855557/churn.all\", schema = schema)"],"metadata":{"collapsed":false,"slideshow":{"slide_type":"fragment"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":["count = churn_df.count()\nvoice_mail_plans = churn_data.filter(churn_data.voice_mail_plan == \" yes\").count()\n\n\"%d, %d\" % (count, voice_mail_plans)"],"metadata":{"collapsed":false,"slideshow":{"slide_type":"fragment"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Intuitively we would find out customers who made one or more customer service calls & How many have more than one?"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["churn_result = churn_df[\"churned\"]"],"metadata":{"collapsed":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":["multiple_service_calls = churn_df.filter(churn_df.number_customer_service_calls > 0).count()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["multiple_service_calls"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["sample_data = churn_df.sample(False, 0.5, 83).toPandas()\nsample_data.head()"],"metadata":{"collapsed":false,"slideshow":{"slide_type":"fragment"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["# Feature Extraction and Model Training\n\n* Code features that are not already numeric\n* Gather all features we need into a single column in the DataFrame.\n* Split labeled data into training and testing set\n* Fit the model to the training data.\n\n## Feature Extraction\nWe need to define our input features."],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\n\nlabel_indexer = StringIndexer(inputCol = 'churned', outputCol = 'label')\nplan_indexer = StringIndexer(inputCol = 'intl_plan', outputCol = 'intl_plan_indexed')\n\nassembler = VectorAssembler(\n    inputCols = ['intl_plan_indexed'] + reduced_numeric_cols,\n    outputCol = 'features')"],"metadata":{"collapsed":false,"slideshow":{"slide_type":"fragment"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Model Training\n\nWe can now define our classifier and pipeline. With this done, we can split our labeled data in train and test sets and fit a model."],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"markdown","source":["#DecisionTreeClassifier"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(labelCol = 'label', featuresCol = 'features')\n\npipeline = Pipeline(stages=[plan_indexer, label_indexer, assembler, classifier])\n\n(train, test) = churn_data.randomSplit([0.7, 0.3])\nmodel = pipeline.fit(train)"],"metadata":{"collapsed":false,"scrolled":true,"slideshow":{"slide_type":"fragment"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Model Evaluation"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\npredictions = model.transform(test)\nevaluator = BinaryClassificationEvaluator()\nauroc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\naupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n\"The AUROC is %s and the AUPR is %s.\" % (auroc, aupr)"],"metadata":{"collapsed":false,"slideshow":{"slide_type":"subslide"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["#RandomForestClassifier"],"metadata":{"slideshow":{"slide_type":"subslide"}}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\nclassifier = RandomForestClassifier(labelCol= 'label', featuresCol= 'features', maxDepth= 10, numTrees= 100, impurity = 'entropy')\n\npipeline = Pipeline(stages=[plan_indexer, label_indexer, assembler, classifier])\n\n(train, test) = churn_data.randomSplit([0.7, 0.3])\n\nmodel = pipeline.fit(train)"],"metadata":{"collapsed":true,"slideshow":{"slide_type":"fragment"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Model Evaluation"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\npredictions = model.transform(test)\nevaluator = BinaryClassificationEvaluator()\nauroc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\naupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n\"The AUROC is %s and the AUPR is %s.\" % (auroc, aupr)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":19}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2.0},"version":"2.7.6","nbconvert_exporter":"python","file_extension":".py"},"name":"ds-for-telco-with-output","notebookId":12845,"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"celltoolbar":"Slideshow"},"nbformat":4,"nbformat_minor":0}
